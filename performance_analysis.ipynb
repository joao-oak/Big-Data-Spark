{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/29 16:51:51 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark = (SparkSession.builder\n",
    "    .master(\"local\")\n",
    "    .appName(\"SQLite JDBC\")\n",
    "    .config(\n",
    "        \"spark.jars\",\n",
    "        \"{}/sqlite-jdbc-3.34.0.jar\".format(os.getcwd()))\n",
    "    .config(\n",
    "        \"spark.driver.extraClassPath\",\n",
    "        \"{}/sqlite-jdbc-3.34.0.jar\".format(os.getcwd()))\n",
    "    .config(\"spark.executor.cores\", '8')\n",
    "    .config(\"spark.sql.compute.ops_on_diff_frames\", True) \n",
    "    .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinevas/.asdf/installs/python/3.10.13/lib/python3.10/site-packages/pyspark/pandas/__init__.py:50: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pyspark.pandas as ps\n",
    "\n",
    "con=\"jdbc:sqlite:{}/database.sqlite\".format(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ranking_date</th>\n",
       "      <th>rank</th>\n",
       "      <th>player</th>\n",
       "      <th>points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20000110</td>\n",
       "      <td>1</td>\n",
       "      <td>101736</td>\n",
       "      <td>4135.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20000110</td>\n",
       "      <td>2</td>\n",
       "      <td>102338</td>\n",
       "      <td>2915.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ranking_date  rank  player  points\n",
       "0      20000110     1  101736  4135.0\n",
       "1      20000110     2  102338  2915.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rankings = ps.read_sql(\"rankings\", con=con)\n",
    "# rankings = rankings.rename(columns={'player': 'player_id'})\n",
    "rankings.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>player_id</th>\n",
       "      <th>name_first</th>\n",
       "      <th>name_last</th>\n",
       "      <th>hand</th>\n",
       "      <th>dob</th>\n",
       "      <th>ioc</th>\n",
       "      <th>height</th>\n",
       "      <th>wikidata_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100001</td>\n",
       "      <td>Gardnar</td>\n",
       "      <td>Mulloy</td>\n",
       "      <td>R</td>\n",
       "      <td>19131122.0</td>\n",
       "      <td>USA</td>\n",
       "      <td>185.0</td>\n",
       "      <td>Q54544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100002</td>\n",
       "      <td>Pancho</td>\n",
       "      <td>Segura</td>\n",
       "      <td>R</td>\n",
       "      <td>19210620.0</td>\n",
       "      <td>ECU</td>\n",
       "      <td>168.0</td>\n",
       "      <td>Q54581</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   player_id name_first name_last hand         dob  ioc  height wikidata_id\n",
       "0     100001    Gardnar    Mulloy    R  19131122.0  USA   185.0      Q54544\n",
       "1     100002     Pancho    Segura    R  19210620.0  ECU   168.0      Q54581"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "players = ps.read_sql(\"players\", con=con)\n",
    "players.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tourney_id</th>\n",
       "      <th>tourney_name</th>\n",
       "      <th>surface</th>\n",
       "      <th>draw_size</th>\n",
       "      <th>tourney_level</th>\n",
       "      <th>tourney_date</th>\n",
       "      <th>match_num</th>\n",
       "      <th>winner_id</th>\n",
       "      <th>winner_seed</th>\n",
       "      <th>winner_entry</th>\n",
       "      <th>winner_name</th>\n",
       "      <th>winner_hand</th>\n",
       "      <th>winner_ht</th>\n",
       "      <th>winner_ioc</th>\n",
       "      <th>winner_age</th>\n",
       "      <th>loser_id</th>\n",
       "      <th>loser_seed</th>\n",
       "      <th>loser_entry</th>\n",
       "      <th>loser_name</th>\n",
       "      <th>loser_hand</th>\n",
       "      <th>loser_ht</th>\n",
       "      <th>loser_ioc</th>\n",
       "      <th>loser_age</th>\n",
       "      <th>score</th>\n",
       "      <th>best_of</th>\n",
       "      <th>round</th>\n",
       "      <th>minutes</th>\n",
       "      <th>w_ace</th>\n",
       "      <th>w_df</th>\n",
       "      <th>w_svpt</th>\n",
       "      <th>w_1stIn</th>\n",
       "      <th>w_1stWon</th>\n",
       "      <th>w_2ndWon</th>\n",
       "      <th>w_SvGms</th>\n",
       "      <th>w_bpSaved</th>\n",
       "      <th>w_bpFaced</th>\n",
       "      <th>l_ace</th>\n",
       "      <th>l_df</th>\n",
       "      <th>l_svpt</th>\n",
       "      <th>l_1stIn</th>\n",
       "      <th>l_1stWon</th>\n",
       "      <th>l_2ndWon</th>\n",
       "      <th>l_SvGms</th>\n",
       "      <th>l_bpSaved</th>\n",
       "      <th>l_bpFaced</th>\n",
       "      <th>winner_rank</th>\n",
       "      <th>winner_rank_points</th>\n",
       "      <th>loser_rank</th>\n",
       "      <th>loser_rank_points</th>\n",
       "      <th>winner1_id</th>\n",
       "      <th>winner2_id</th>\n",
       "      <th>loser1_id</th>\n",
       "      <th>loser2_id</th>\n",
       "      <th>winner1_name</th>\n",
       "      <th>winner1_hand</th>\n",
       "      <th>winner1_ht</th>\n",
       "      <th>winner1_ioc</th>\n",
       "      <th>winner1_age</th>\n",
       "      <th>winner2_name</th>\n",
       "      <th>winner2_hand</th>\n",
       "      <th>winner2_ht</th>\n",
       "      <th>winner2_ioc</th>\n",
       "      <th>winner2_age</th>\n",
       "      <th>loser1_name</th>\n",
       "      <th>loser1_hand</th>\n",
       "      <th>loser1_ht</th>\n",
       "      <th>loser1_ioc</th>\n",
       "      <th>loser1_age</th>\n",
       "      <th>loser2_name</th>\n",
       "      <th>loser2_hand</th>\n",
       "      <th>loser2_ht</th>\n",
       "      <th>loser2_ioc</th>\n",
       "      <th>loser2_age</th>\n",
       "      <th>winner1_rank</th>\n",
       "      <th>winner1_rank_points</th>\n",
       "      <th>winner2_rank</th>\n",
       "      <th>winner2_rank_points</th>\n",
       "      <th>loser1_rank</th>\n",
       "      <th>loser1_rank_points</th>\n",
       "      <th>loser2_rank</th>\n",
       "      <th>loser2_rank_points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-M020</td>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Hard</td>\n",
       "      <td>32</td>\n",
       "      <td>A</td>\n",
       "      <td>20170102.0</td>\n",
       "      <td>271</td>\n",
       "      <td>104678.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Viktor Troicki</td>\n",
       "      <td>R</td>\n",
       "      <td>193.0</td>\n",
       "      <td>SRB</td>\n",
       "      <td>30.8</td>\n",
       "      <td>106415.0</td>\n",
       "      <td>None</td>\n",
       "      <td>Q</td>\n",
       "      <td>Yoshihito Nishioka</td>\n",
       "      <td>L</td>\n",
       "      <td>170.0</td>\n",
       "      <td>JPN</td>\n",
       "      <td>21.2</td>\n",
       "      <td>6-4 7-5</td>\n",
       "      <td>3</td>\n",
       "      <td>R32</td>\n",
       "      <td>91.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1385.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>604.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-M020</td>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Hard</td>\n",
       "      <td>32</td>\n",
       "      <td>A</td>\n",
       "      <td>20170102.0</td>\n",
       "      <td>272</td>\n",
       "      <td>106378.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Kyle Edmund</td>\n",
       "      <td>R</td>\n",
       "      <td>188.0</td>\n",
       "      <td>GBR</td>\n",
       "      <td>21.9</td>\n",
       "      <td>124014.0</td>\n",
       "      <td>None</td>\n",
       "      <td>Q</td>\n",
       "      <td>Ernesto Escobedo</td>\n",
       "      <td>R</td>\n",
       "      <td>185.0</td>\n",
       "      <td>USA</td>\n",
       "      <td>20.4</td>\n",
       "      <td>7-6(4) 7-6(6)</td>\n",
       "      <td>3</td>\n",
       "      <td>R32</td>\n",
       "      <td>130.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>443.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  tourney_id tourney_name surface draw_size tourney_level  tourney_date match_num  winner_id winner_seed winner_entry     winner_name winner_hand  winner_ht winner_ioc  winner_age  loser_id loser_seed loser_entry          loser_name loser_hand  loser_ht loser_ioc  loser_age          score best_of round  minutes  w_ace  w_df  w_svpt  w_1stIn  w_1stWon  w_2ndWon  w_SvGms  w_bpSaved  w_bpFaced  l_ace  l_df  l_svpt  l_1stIn  l_1stWon  l_2ndWon  l_SvGms  l_bpSaved  l_bpFaced  winner_rank  winner_rank_points  loser_rank  loser_rank_points  winner1_id winner2_id loser1_id  loser2_id winner1_name  winner1_hand winner1_ht  winner1_ioc winner1_age winner2_name  winner2_hand winner2_ht  winner2_ioc winner2_age loser1_name  loser1_hand loser1_ht  loser1_ioc loser1_age loser2_name  loser2_hand loser2_ht  loser2_ioc  loser2_age  winner1_rank  winner1_rank_points  winner2_rank  winner2_rank_points  loser1_rank  loser1_rank_points  loser2_rank  loser2_rank_points\n",
       "0  2017-M020     Brisbane    Hard        32             A    20170102.0       271   104678.0        None         None  Viktor Troicki           R      193.0        SRB        30.8  106415.0       None           Q  Yoshihito Nishioka          L     170.0       JPN       21.2        6-4 7-5       3   R32     91.0   11.0   5.0    64.0     45.0      35.0       6.0     11.0        1.0        3.0    0.0   1.0    82.0     53.0      33.0      13.0     11.0        6.0       10.0         29.0              1385.0       100.0              604.0         NaN       None      None        NaN         None           NaN       None          NaN        None         None           NaN       None          NaN        None        None          NaN      None         NaN       None        None          NaN      None         NaN         NaN           NaN                  NaN           NaN                  NaN          NaN                 NaN          NaN                 NaN\n",
       "1  2017-M020     Brisbane    Hard        32             A    20170102.0       272   106378.0        None         None     Kyle Edmund           R      188.0        GBR        21.9  124014.0       None           Q    Ernesto Escobedo          R     185.0       USA       20.4  7-6(4) 7-6(6)       3   R32    130.0   11.0   2.0    83.0     48.0      37.0      19.0     12.0        2.0        3.0   11.0   3.0   113.0     67.0      39.0      27.0     12.0        9.0       10.0         45.0              1001.0       141.0              443.0         NaN       None      None        NaN         None           NaN       None          NaN        None         None           NaN       None          NaN        None        None          NaN      None         NaN       None        None          NaN      None         NaN         NaN           NaN                  NaN           NaN                  NaN          NaN                 NaN          NaN                 NaN"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches = ps.read_sql('matches', con=con)\n",
    "# matches = matches.rename(columns={'winner_id': 'player_id'})\n",
    "# matches.dropna()\n",
    "# matches['winner_id'] = matches['winner_id'].astype(int)\n",
    "matches.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinevas/.asdf/installs/python/3.10.13/lib/python3.10/site-packages/pyspark/pandas/utils.py:1016: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `to_spark`, the existing index is lost when converting to Spark DataFrame.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    }
   ],
   "source": [
    "rankings = rankings.to_spark()\n",
    "players = players.to_spark()\n",
    "matches = matches.to_spark()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "count the number of matches and average rank of players with rankings above 100\n",
    "\n",
    "o tempo deu sempre o mesmo para 2, 4 e 8 cores..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT players.name_first, players.name_last, \n",
    "       AVG(rankings.rank) AS avg_rank, \n",
    "       COUNT(matches.match_num) AS total_matches\n",
    "FROM players\n",
    "JOIN rankings ON players.player_id = rankings.player\n",
    "JOIN matches ON players.player_id = matches.winner_id\n",
    "WHERE rankings.rank <= 100\n",
    "GROUP BY players.name_first, players.name_last\n",
    "ORDER BY avg_rank DESC\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT players.name_first, players.name_last, \n",
    "       AVG(rankings.rank) AS avg_rank, \n",
    "       COUNT(matches.match_num) AS total_matches\n",
    "FROM players\n",
    "JOIN rankings ON players.player_id = rankings.player\n",
    "JOIN matches ON players.player_id = matches.winner_id\n",
    "GROUP BY players.name_first, players.name_last\n",
    "ORDER BY avg_rank DESC\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT ranking_date, AVG(rank) AS avg_rank, SUM(points) AS total_points\n",
    "FROM rankings_1\n",
    "GROUP BY ranking_date\n",
    "UNION\n",
    "SELECT ranking_date, AVG(rank) AS avg_rank, SUM(points) AS total_points\n",
    "FROM rankings_2\n",
    "GROUP BY ranking_date;\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name_first</th>\n",
       "      <th>name_last</th>\n",
       "      <th>avg_rank</th>\n",
       "      <th>total_matches</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alex</td>\n",
       "      <td>Lopez Moron</td>\n",
       "      <td>100.000000000000000000</td>\n",
       "      <td>536.000000000000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Francisco</td>\n",
       "      <td>Montana</td>\n",
       "      <td>100.000000000000000000</td>\n",
       "      <td>91.000000000000000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  name_first    name_last                avg_rank           total_matches\n",
       "0       Alex  Lopez Moron  100.000000000000000000  536.000000000000000000\n",
       "1  Francisco      Montana  100.000000000000000000   91.000000000000000000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "df = ps.read_sql_query(query, con=con)\n",
    "print(time.time() - start_time)\n",
    "# df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# query 3M spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, sum\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Rankings Aggregation\") \\\n",
    "    .config(\n",
    "        \"spark.driver.extraClassPath\",\n",
    "        \"{}/sqlite-jdbc-3.34.0.jar\".format(os.getcwd())) \\\n",
    "    .config(\"spark.executor.cores\", '1') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "start_time = time.time()\n",
    "# Load the rankings data into DataFrames (replace \"path_to_rankings_1\" and \"path_to_rankings_2\" with actual file paths)\n",
    "url = \"jdbc:sqlite:{}/database.sqlite\".format(os.getcwd())\n",
    "\n",
    "rankings_df1 = spark.read.jdbc(url=url, table=\"rankings\")\n",
    "rankings_df2 = spark.read.jdbc(url=url, table=\"rankings\")\n",
    "\n",
    "# Apply aggregation functions\n",
    "aggregated_df1 = rankings_df1.groupBy(\"ranking_date\") \\\n",
    "    .agg(avg(\"rank\").alias(\"avg_rank\"), sum(\"points\").alias(\"total_points\"))\n",
    "\n",
    "aggregated_df2 = rankings_df2.groupBy(\"ranking_date\") \\\n",
    "    .agg(avg(\"rank\").alias(\"avg_rank\"), sum(\"points\").alias(\"total_points\"))\n",
    "\n",
    "# Union the two DataFrames\n",
    "result_df = aggregated_df1.union(aggregated_df2)\n",
    "\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/29 16:22:38 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----------------+-------------+\n",
      "|name_first|   name_last|         avg_rank|total_matches|\n",
      "+----------+------------+-----------------+-------------+\n",
      "|      Jeff| Salzenstein|            100.0|          212|\n",
      "|     Terry|     Addison|            100.0|           71|\n",
      "| Francisco|     Montana|            100.0|           91|\n",
      "|     Terry|        Ryan|            100.0|           52|\n",
      "|      Alex| Lopez Moron|            100.0|          536|\n",
      "|      Paul|      Wekesa|            100.0|          393|\n",
      "|Juan Pablo|      Guzman|            100.0|          828|\n",
      "|     Jurij|    Rodionov|            100.0|          454|\n",
      "|     Melle|Van Gemerden|            100.0|          396|\n",
      "|  Vincenzo|  Santopadre|            100.0|          313|\n",
      "|      Hugo|     Armando|            100.0|          290|\n",
      "|John Chris|       Lewis|            100.0|           20|\n",
      "|      Tony|      Graham|            100.0|           42|\n",
      "|      Luca|       Vanni|            100.0|          912|\n",
      "|    Emilio|     Montano|            99.75|          104|\n",
      "|     Pedro|       Sousa|             99.5|          854|\n",
      "|      Axel|    Pretzsch|99.33333333333333|          564|\n",
      "|   Michael|    Kohlmann|            99.25|         1052|\n",
      "|      Dick|  Bohrnstedt|             99.0|          104|\n",
      "|    Gilles|    Elseneer|             99.0|         3263|\n",
      "+----------+------------+-----------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, avg, count\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "    .master(\"local\")\n",
    "    .appName(\"SQLite JDBC\")\n",
    "    .config(\n",
    "        \"spark.jars\",\n",
    "        \"{}/sqlite-jdbc-3.34.0.jar\".format(os.getcwd()))\n",
    "    .config(\n",
    "        \"spark.driver.extraClassPath\",\n",
    "        \"{}/sqlite-jdbc-3.34.0.jar\".format(os.getcwd()))\n",
    "    .config(\"spark.executor.cores\", '1')\n",
    "    .config(\"spark.sql.compute.ops_on_diff_frames\", True) \n",
    "    .getOrCreate())\n",
    "\n",
    "total_time = []\n",
    "for i in range(1000):\n",
    "    start_time = time.time()\n",
    "    url = \"jdbc:sqlite:{}/database.sqlite\".format(os.getcwd())\n",
    "    \n",
    "    players_df = spark.read.jdbc(url=url, table=\"players\")\n",
    "    rankings_df = spark.read.jdbc(url=url, table=\"rankings\")\n",
    "    matches_df = spark.read.jdbc(url=url, table=\"matches\")\n",
    "    \n",
    "    # Perform joins and filters\n",
    "    merged_df = players_df.join(rankings_df, players_df[\"player_id\"] == rankings_df[\"player\"]) \\\n",
    "        .join(matches_df, players_df[\"player_id\"] == matches_df[\"winner_id\"]) \\\n",
    "        .filter(rankings_df[\"rank\"] <= 100)\n",
    "    \n",
    "    # Perform aggregation and ordering\n",
    "    result_df = merged_df.groupBy(\"name_first\", \"name_last\") \\\n",
    "        .agg(avg(\"rank\").alias(\"avg_rank\"), count(\"match_num\").alias(\"total_matches\")) \\\n",
    "        .orderBy(col(\"avg_rank\").desc())\n",
    "    \n",
    "    total_time.append(time.time() - start_time)\n",
    "\n",
    "print(sum(total_time) / len(total_time))\n",
    "# # Show the result\n",
    "# result_df.show()\n",
    "\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# query 6M sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT r1.ranking_date AS date1,\n",
    "       r1.rank AS rank1,\n",
    "       r1.points AS points1,\n",
    "       r2.ranking_date AS date2,\n",
    "       r2.rank AS rank2,\n",
    "       r2.points AS points2\n",
    "FROM rankings r1\n",
    "JOIN rankings r2 ON r1.ranking_date < r2.ranking_date\n",
    "'''\n",
    "\n",
    "spark.stop()\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "    .master(\"local\")\n",
    "    .appName(\"SQLite JDBC\")\n",
    "    .config(\n",
    "        \"spark.jars\",\n",
    "        \"{}/sqlite-jdbc-3.34.0.jar\".format(os.getcwd()))\n",
    "    .config(\n",
    "        \"spark.driver.extraClassPath\",\n",
    "        \"{}/sqlite-jdbc-3.34.0.jar\".format(os.getcwd()))\n",
    "    .config(\"spark.executor.cores\", '1')\n",
    "    .config(\"spark.sql.compute.ops_on_diff_frames\", True) \n",
    "    .getOrCreate())\n",
    "\n",
    "import pyspark.pandas as ps\n",
    "\n",
    "con=\"jdbc:sqlite:{}/database.sqlite\".format(os.getcwd())\n",
    "\n",
    "start_time = time.time()\n",
    "df = ps.read_sql_query(query, con=con)\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# query 6M spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = (SparkSession.builder\n",
    "    .master(\"local\")\n",
    "    .appName(\"SQLite JDBC\")\n",
    "    .config(\n",
    "        \"spark.jars\",\n",
    "        \"{}/sqlite-jdbc-3.34.0.jar\".format(os.getcwd()))\n",
    "    .config(\n",
    "        \"spark.driver.extraClassPath\",\n",
    "        \"{}/sqlite-jdbc-3.34.0.jar\".format(os.getcwd()))\n",
    "    .config(\"spark.executor.cores\", '8')\n",
    "    .config(\"spark.sql.compute.ops_on_diff_frames\", True) \n",
    "    .getOrCreate())\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Load the rankings data into a DataFrame (replace \"path_to_rankings\" with the actual file path)\n",
    "url = \"jdbc:sqlite:{}/database.sqlite\".format(os.getcwd())\n",
    "rankings_df = spark.read.jdbc(url=url, table=\"rankings\")\n",
    "\n",
    "rankings_df.repartition(7)\n",
    "\n",
    "# Self-join the rankings DataFrame\n",
    "rankings_df = rankings_df.alias(\"r1\") \\\n",
    "    .join(rankings_df.alias(\"r2\"), col(\"r1.ranking_date\") < col(\"r2.ranking_date\")) \\\n",
    "    .select(col(\"r1.ranking_date\").alias(\"date1\"),\n",
    "            col(\"r1.rank\").alias(\"rank1\"),\n",
    "            col(\"r1.points\").alias(\"points1\"),\n",
    "            col(\"r2.ranking_date\").alias(\"date2\"),\n",
    "            col(\"r2.rank\").alias(\"rank2\"),\n",
    "            col(\"r2.points\").alias(\"points2\"))\n",
    "\n",
    "print(time.time() - start_time)\n",
    "\n",
    "spark.stop() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: org.apache.spark.SparkException: Only one SparkContext should be running in this JVM (see SPARK-2243).The currently running SparkContext was created at:\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:829)\n\tat org.apache.spark.SparkContext$.$anonfun$assertNoOtherContextIsRunning$2(SparkContext.scala:2845)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.SparkContext$.assertNoOtherContextIsRunning(SparkContext.scala:2842)\n\tat org.apache.spark.SparkContext$.markPartiallyConstructed(SparkContext.scala:2932)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:99)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat jdk.internal.reflect.GeneratedConstructorAccessor96.newInstance(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m avg, col, count\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Initialize SparkContext\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlocal\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSQLite RDD Example\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Initialize SparkSession\u001b[39;00m\n\u001b[1;32m      9\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession(sc)\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.10.13/lib/python3.10/site-packages/pyspark/context.py:203\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    201\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mappName\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43msparkHome\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpyFiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatchSize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjsc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mudf_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.10.13/lib/python3.10/site-packages/pyspark/context.py:296\u001b[0m, in \u001b[0;36mSparkContext._do_init\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    295\u001b[0m \u001b[38;5;66;03m# Create the Java SparkContext through Py4J\u001b[39;00m\n\u001b[0;32m--> 296\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc \u001b[38;5;241m=\u001b[39m jsc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_context\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;66;03m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conf \u001b[38;5;241m=\u001b[39m SparkConf(_jconf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc()\u001b[38;5;241m.\u001b[39mconf())\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.10.13/lib/python3.10/site-packages/pyspark/context.py:421\u001b[0m, in \u001b[0;36mSparkContext._initialize_context\u001b[0;34m(self, jconf)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;124;03mInitialize SparkContext in function to allow subclass specific initialization\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mJavaSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjconf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.10.13/lib/python3.10/site-packages/py4j/java_gateway.py:1587\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1581\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1582\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_command_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1583\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1584\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1586\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1587\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1588\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fqn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1590\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1591\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.10.13/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.10.13/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: org.apache.spark.SparkException: Only one SparkContext should be running in this JVM (see SPARK-2243).The currently running SparkContext was created at:\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:829)\n\tat org.apache.spark.SparkContext$.$anonfun$assertNoOtherContextIsRunning$2(SparkContext.scala:2845)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.SparkContext$.assertNoOtherContextIsRunning(SparkContext.scala:2842)\n\tat org.apache.spark.SparkContext$.markPartiallyConstructed(SparkContext.scala:2932)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:99)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat jdk.internal.reflect.GeneratedConstructorAccessor96.newInstance(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, col, count\n",
    "\n",
    "# Initialize SparkContext\n",
    "# sc = SparkContext(\"local\", \"SQLite RDD Example\")\n",
    "\n",
    "# Initialize SparkSession\n",
    "# spark = SparkSession(sc)\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Define the file paths\n",
    "players_path = \"file:///{}/database.sqlite\".format(os.getcwd())\n",
    "rankings_path = \"file:///{}/database.sqlite\".format(os.getcwd())\n",
    "matches_path = \"file:///{}/database.sqlite\".format(os.getcwd())\n",
    "\n",
    "# Load data into RDDs\n",
    "players_rdd = sc.textFile(players_path)\n",
    "rankings_rdd = sc.textFile(rankings_path)\n",
    "matches_rdd = sc.textFile(matches_path)\n",
    "\n",
    "# Transform RDDs into key-value pairs\n",
    "players_kv = players_rdd.map(lambda line: line.split(\",\")).map(lambda fields: (fields[0], (fields[1], fields[2])))\n",
    "rankings_kv = rankings_rdd.map(lambda line: line.split(\",\")).map(lambda fields: (fields[0], fields[1]))\n",
    "matches_kv = matches_rdd.map(lambda line: line.split(\",\")).map(lambda fields: (fields[0], fields[1]))\n",
    "\n",
    "# Perform the joins and filters\n",
    "merged_rdd = players_kv.join(rankings_kv).map(lambda x: (x[0], (x[1][0][0], x[1][0][1], x[1][1])))\n",
    "merged_rdd = merged_rdd.join(matches_kv).map(lambda x: (x[1][0][0], x[1][0][1], x[1][0][2], x[1][1]))\n",
    "\n",
    "# Filter based on rankings rank\n",
    "filtered_rdd = merged_rdd.filter(lambda x: int(x[2]) <= 100)\n",
    "\n",
    "# Perform aggregation\n",
    "aggregated_rdd = filtered_rdd.map(lambda x: ((x[0], x[1]), (int(x[2]), 1))) \\\n",
    "    .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1])) \\\n",
    "    .mapValues(lambda x: (x[0] / x[1], x[1]))\n",
    "\n",
    "# Convert RDD to DataFrame for sorting\n",
    "# result_df = aggregated_rdd.map(lambda x: (x[0][0], x[0][1], x[1][0], x[1][1])).toDF([\"name_first\", \"name_last\", \"avg_rank\", \"total_matches\"])\n",
    "\n",
    "# # Order by avg_rank in descending order\n",
    "# result_df = result_df.orderBy(col(\"avg_rank\").desc())\n",
    "\n",
    "# Show the result\n",
    "aggregated_rdd.take(10)\n",
    "\n",
    "# Stop SparkSession\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shims",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
